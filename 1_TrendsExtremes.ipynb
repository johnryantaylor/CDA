{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-sjlCZwLotO"
      },
      "source": [
        "# ERA5 reanalysis\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This notebook provides an introduction to download and working with the ERA5 *reanalysis* dataset. A reanalysis is created by assimilating historical observations into a global climate model to produce a physically consistent estimate of the past atmospheric state. The ERA5 dataset is extremely useful for various problems in climate science. It was also used to train AI forecast models like Graphcast.\n",
        "\n",
        "### What is ERA5?\n",
        "\n",
        "ERA5 is the fifth generation ECMWF (European Centre for Medium-Range Weather Forecasts) atmospheric reanalysis of the global climate. It provides:\n",
        "\n",
        "- Hourly estimates of atmospheric, land, and oceanic climate variables\n",
        "- Global coverage at 0.25° × 0.25° resolution (~31 km)\n",
        "- Data from 1940 to present (with about 5-day latency)\n",
        "- Consistent, gap-free estimate of recent climate state\n",
        "\n",
        "### ERA5 Data Temporal Resolutions\n",
        "\n",
        "ERA5 data is available at various temporal resolutions. Here, we will work with two temporal resolutions. `reanalysis-era5-single-levels` contains data saved at hourly temporal resolution, and `reanalysis-era5-single-levels-monthly-means` contains data saved at monthly resolution. It is of course possible to calculate monthly averages from hourly resolution data, but this would be much slower and would require more memory and/or disk space."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-x5RH8TDLotP"
      },
      "source": [
        "## Setup and Installation\n",
        "\n",
        "Before we start we need to install some required packages. The cell below will install the required packages.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-ittEF4LotP"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install -q earthkit-data cdsapi xarray netcdf4 matplotlib cartopy numpy regionmask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjM12azpLotQ"
      },
      "source": [
        "Now, load the packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J5EtDt4ZLotQ"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import xarray as xr\n",
        "import matplotlib.pyplot as plt\n",
        "import cartopy.crs as ccrs\n",
        "import cartopy.feature as cfeature\n",
        "import earthkit.data\n",
        "import regionmask\n",
        "\n",
        "print(\"All libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mn_L5IpLotQ"
      },
      "source": [
        "## CDS API Configuration\n",
        "\n",
        "To download ERA5 data, you need a free account on the **Climate Data Store (CDS)**.\n",
        "\n",
        "### How to get your API key:\n",
        "\n",
        "1. Go to https://cds.climate.copernicus.eu/\n",
        "2. Create a free account or log in\n",
        "3. Click on your name in the top right → select \"Profile\"\n",
        "4. Find your API key and click the copy icon\n",
        "5. Paste it in the code cell below\n",
        "\n",
        "**Important:** You must also accept the dataset terms by visiting:\n",
        "- https://cds.climate.copernicus.eu/datasets/reanalysis-era5-single-levels-monthly-means\n",
        "- https://cds.climate.copernicus.eu/datasets/reanalysis-era5-single-levels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-mNXg9WvLotQ"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# ENTER YOUR CDS API KEY HERE\n",
        "# ==========================================\n",
        "CDS_URL = 'https://cds.climate.copernicus.eu/api'\n",
        "CDS_API_KEY = '6bb7b4ff-392c-4937-a0ef-0bdce7fde8a8'  # Replace with your actual API key (format: xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx)\n",
        "\n",
        "# Set as environment variables for earthkit\n",
        "os.environ['CDSAPI_URL'] = CDS_URL\n",
        "os.environ['CDSAPI_KEY'] = CDS_API_KEY\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJEhOt_LLotQ"
      },
      "source": [
        "---\n",
        "\n",
        "## Introduction to xarray\n",
        "\n",
        "xarray is a Python library that makes working with multi-dimensional labeled arrays intuitive and efficient. It's particularly well-suited for climate data. See https://docs.xarray.dev/en/stable/index.html for more detailed information.\n",
        "\n",
        "### File formats\n",
        "\n",
        "Xarray allows us to read data stored in a wide range of file formats, including NetCDF, HDF5, zarr, GRIB, and OPeNDAP which are commonly used for climate data.\n",
        "\n",
        "### Data labelling\n",
        "\n",
        "Unlike numpy arrays, xarray allows us to add labels to our dataset which make it much easier to work with.\n",
        "\n",
        "- **Labeled dimensions**: Instead of remembering what the first and second array dimension correspond to (as in a numpy array), xarray allows us to name the dimensions of an array (e.g. time, latitude, height, etc.)\n",
        "\n",
        "- **Coordinates**: Data can be associated with coordinate values (e.g., actual dates, lat/lon values), not just integer indices like in numpy arrays.\n",
        "\n",
        "- **Metadata preservation**: We can add metadata to keep track of units, add descriptions, and other assign other attributes to the data\n",
        "\n",
        "### Data Chunking\n",
        "\n",
        "Climate datasets can be enormous (petabytes!). **Chunking** divides data into smaller pieces that can be loaded on demand. This makes it much faster to read and operate on data in cases where we don't need all of the data for our particular analysis. Chunks can be arranged in space or time for doing spatial analysis (e.g. mapping) or in time for timeseries analysis.\n",
        "\n",
        "### Data structures\n",
        "\n",
        "xarray has two main data structures:\n",
        "\n",
        "1. **Dataset**: A collection of multiple variables (DataArrays) that share dimensions\n",
        "   - Like a Python dictionary of DataArrays\n",
        "   - Example: A file containing temperature, pressure, and precipitation as functions of latitude, longitude, and time.\n",
        "   \n",
        "2. **DataArray**: A single multi-dimensional variable with dimensions and coordinates\n",
        "   - Analogous to a NumPy array, but with labels\n",
        "   - Example: Ground temperature as a function of latitude, longitude and time.\n",
        "\n",
        "`dataset['temperature']` or `dataset.temperature` where `dataset` is the name of a Dataset returns a DataArray\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bww6BKQ_LotR"
      },
      "source": [
        "### Built-in Xarray operations\n",
        "\n",
        "xarray provides a rich set of built-in operations for working with labeled multi-dimensional arrays. Here are some particularly useful operations for analyzing climate data. The operations can be applied to DataArrays using the name of the DataArray followed by . and then the name of the operation.\n",
        "\n",
        "#### **Selection and Indexing**\n",
        "- **`.sel()`**: Select by coordinate label (e.g., `data.sel(time='2024-01-01', lat=51.5)` where `data` is the name of a DataArray selects data at the specified date and latitude)\n",
        "- **`.sel(method='nearest')`**: Find nearest grid point to a coordinate value\n",
        "- **`.squeeze()`**: Remove dimensions of length 1 (singleton dimensions)\n",
        "- **`.values`**: Access the underlying NumPy array\n",
        "- **`.dims`**: Get dimension names\n",
        "- **`.coords`**: Access coordinate arrays\n",
        "\n",
        "#### **Aggregation Operations**\n",
        "- **`.mean()`**: Calculate mean along dimension(s)\n",
        "- **`.sum()`**: Calculate sum along dimension(s)\n",
        "- **`.std()`**: Calculate standard deviation\n",
        "- **`.min()`**, **`.max()`**: Find minimum/maximum values\n",
        "- **`.argmax()`**, **`.argmin()`**: Find indices of maximum/minimum values\n",
        "\n",
        "#### **Temporal Operations**\n",
        "- **`.resample()`**: Change temporal resolution (e.g., `data.resample(time='1D').mean()`)\n",
        "- **`.rolling()`**: Rolling window operations (e.g., `data.rolling(time=12).mean()` takes the rolling annual average when applied to monthly data)\n",
        "- **`.groupby()`**: Group by dimension or coordinate for subsequent operations (e.g., `data.groupby('time.month').mean()` averages data with the same month (e.g. January).)\n",
        "\n",
        "#### **Mathematical Operations**\n",
        "- **`.polyfit()`**: Fit polynomial trends (e.g., `data.polyfit(dim='time', deg=1)` fits a line to the data)\n",
        "- **`.transpose()`**: Changes the order of the dimensions (e.g., `data.transpose('time', 'lat', 'lon')` changes the order of the DataArray so that time comes first, followed by latitude and then longitude). This is sometimes needed for operations that expect the spatiotemporal data in a certain order.\n",
        "\n",
        "#### **Data Manipulation**\n",
        "- **`.plot()`**: Built-in plotting with automatic coordinate handling\n",
        "- **Arithmetic operations**: Xarray can do direct math operations (e.g., `data - 273.15`, `data1 + data2`)\n",
        "\n",
        "#### **Other Common Operations** (useful for advanced analysis)\n",
        "- **`.where()`**: Conditional selection (e.g., `data.where(data > 0)`)\n",
        "- **`.interp()`**: Interpolate to new coordinates\n",
        "- **`.assign()`**: Add new variables to Dataset (e.g., `ds.assign(temp_c=data - 273.15)`)\n",
        "- **`.rename()`**: Rename dimensions or variables\n",
        "- **`.drop()`**: Remove (drop) dimensions or variables\n",
        "- **`.stack()`** / **`.unstack()`**: Reshape dimensions (stack creates MultiIndex, unstack expands it)\n",
        "- **`.load()`** / **`.compute()`**: Load/compute lazy (chunked) data into memory\n",
        "- **`.to_netcdf()`** / **`.to_zarr()`**: Save to netcdf or zarr files\n",
        "- **`.copy()`**: Create a copy of the data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GX-pLo7_LotR"
      },
      "source": [
        "---\n",
        "\n",
        "### Example 1: Surface temperature at a specified location\n",
        "\n",
        "In this example, we will:\n",
        "1. Download monthly averaged 2m temperature data for a specified location\n",
        "2. Calculate a 12-month rolling average using xarray\n",
        "3. Fit a linear trend over the full timeseries to quantify warming\n",
        "4. Plot the monthly timeseries and the linear warming trend\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vgqb7lakLotR"
      },
      "outputs": [],
      "source": [
        "# Configuration for Example 1\n",
        "# approximate location of Cambridge\n",
        "LAT = 52.20\n",
        "LON = 0.13\n",
        "START_YEAR = 1940\n",
        "END_YEAR = 2024\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygGn73kfLotR"
      },
      "source": [
        "First, we use the `earthkit.data` library to download ERA5 from the Climate Data Store. The monthly means dataset provides pre-computed monthly averages, which is much more efficient than downloading hourly data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FD2f1ZCXLotR"
      },
      "outputs": [],
      "source": [
        "\n",
        "dataset = \"reanalysis-era5-single-levels-monthly-means\"\n",
        "\n",
        "# Create list of years and months\n",
        "years = [str(y) for y in range(START_YEAR, END_YEAR + 1)]\n",
        "months = [f\"{m:02d}\" for m in range(1, 13)]\n",
        "\n",
        "request = {\n",
        "    \"product_type\": \"monthly_averaged_reanalysis\",\n",
        "    \"variable\": [\"2m_temperature\"],\n",
        "    \"year\": years,\n",
        "    \"month\": months,\n",
        "    \"time\": \"00:00\",\n",
        "    \"area\": [LAT, LON, LAT, LON],  # North, West, South, East (single point)\n",
        "    \"data_format\": \"netcdf\",\n",
        "    \"download_format\": \"unarchived\"\n",
        "}\n",
        "\n",
        "print(f\"Downloading monthly data...\")\n",
        "print(\"(This may take 1-2 minutes)\")\n",
        "\n",
        "monthly_data = earthkit.data.from_source(\"cds\", dataset, request).to_xarray()\n",
        "\n",
        "print('Done downloading data')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfUsvKRYLotR"
      },
      "source": [
        "Now that we have downloaded the data, we can look at the dataset using the print command. Notice how xarray provides the dimension names (valid_time, latitude, longitude), coordinates, data variables, and attributes to describe the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7ofDanQLotS"
      },
      "outputs": [],
      "source": [
        "# Examine the xarray Dataset structure\n",
        "print(\"Dataset structure:\")\n",
        "print(monthly_data)\n",
        "\n",
        "# relabel the time dimension to 'time' for convenience\n",
        "monthly_data = monthly_data.rename({'valid_time': 'time'})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itGPJD_QLotS"
      },
      "source": [
        "As a simple example for manipulating the data, let's convert the 2m air temperature from Kelvin to Celsius and find the minimum, maximum, and average (mean) temperature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QlB-rcSzLotS"
      },
      "outputs": [],
      "source": [
        "# Extract 2m temperature and convert to Celsius\n",
        "temp_kelvin = monthly_data.t2m.squeeze()  # Remove singleton lat/lon dimensions\n",
        "temp_celsius = temp_kelvin - 273.15       # Convert K to °C\n",
        "\n",
        "# Get time values\n",
        "times = temp_celsius['time'].values # .values is used to access the underlying numpy array\n",
        "temps = temp_celsius.values\n",
        "\n",
        "print(f\"Temperature range: {temps.min():.1f}°C to {temps.max():.1f}°C\")\n",
        "print(f\"Mean temperature: {temps.mean():.1f}°C\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMTlRZkbLotS"
      },
      "source": [
        "We can use **xarray's rolling** method to perform operations on a rolling group of data. Here, we will do this to calculate a rolling annual average on the monthly data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbswkTXlLotS"
      },
      "outputs": [],
      "source": [
        "# Calculate 12-month rolling average (centered) using xarray\n",
        "# xarray's rolling() works directly on DataArrays with labeled dimensions\n",
        "rolling_avg = temp_celsius.rolling({'time': 12}, center=True).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diXKsmqCLotS"
      },
      "source": [
        "Now, let's use the polyfit function from numpy to fit a linear trend to the timeseries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qHd5B9b4LotS"
      },
      "outputs": [],
      "source": [
        "# Fit linear trend using numpy polyfit\n",
        "# Convert dates to numeric values (months since start)\n",
        "x = np.arange(len(temps))\n",
        "slope, intercept = np.polyfit(x, temps, 1)\n",
        "trend_line = slope * x + intercept\n",
        "\n",
        "# Calculate warming rate per decade\n",
        "# slope is per month, multiply by 12*10 for per decade\n",
        "warming_rate = slope * 120\n",
        "\n",
        "print(f\"Linear trend: {warming_rate:+.3f}°C per decade\")\n",
        "print(f\"Total warming ({START_YEAR}-{END_YEAR}): {warming_rate * (END_YEAR - START_YEAR) / 10:.2f}°C\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5ekSG2ALotS"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Set the default font and figure size (this will apply to later plots too)\n",
        "plt.rcParams['figure.figsize'] = [12, 6]\n",
        "plt.rcParams['font.size'] = 18\n",
        "\n",
        "# Create figure\n",
        "fig, ax = plt.subplots(figsize=(14, 7))\n",
        "\n",
        "# Plot monthly data\n",
        "ax.plot(times, temps, color='#7FB3D5', linewidth=0.8, alpha=0.7,\n",
        "        label='Monthly mean')\n",
        "\n",
        "# Plot 12-month rolling average\n",
        "ax.plot(times, rolling_avg.values, color='#2E86AB', linewidth=2.5,\n",
        "        label='12-month rolling average')\n",
        "\n",
        "# Plot linear trend\n",
        "ax.plot(times, trend_line, color='#E94F37', linewidth=2, linestyle='--',\n",
        "        label=f'Linear trend ({warming_rate:+.2f}°C/decade)')\n",
        "\n",
        "# Formatting\n",
        "ax.set_xlabel('Year')\n",
        "ax.set_ylabel('Temperature (°C)')\n",
        "ax.set_title(f'ERA5 2m height temperature ({LAT}°N, {LON}°E)\\n', fontsize=18, fontweight='bold')\n",
        "\n",
        "ax.legend(loc='upper left', framealpha=0.9)\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Add reference line at 0°C\n",
        "ax.axhline(y=0, color='gray', linestyle=':', linewidth=0.5, alpha=0.5)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('temperature_trend.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nPlot saved as 'temperature_trend.png'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3UD_ySxLotS"
      },
      "source": [
        "### Warming stripes\n",
        "\n",
        "Just for fun, let's plot the `warming stripes' at our location. (\"Warming stripes\" are a minimalist visualization created by climatologist Ed Hawkins that shows temperature changes over time using only color.)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJe4QURlLotS"
      },
      "outputs": [],
      "source": [
        "# Calculate annual mean temperatures from monthly data\n",
        "# First, add a 'year' coordinate to group by\n",
        "temp_with_year = temp_celsius.assign_coords(year=('time', temp_celsius['time'].dt.year.data))\n",
        "\n",
        "# Calculate annual average for each year\n",
        "annual_temps = temp_with_year.groupby('year').mean()\n",
        "\n",
        "# Calculate anomalies (departure from long-term mean)\n",
        "mean_temp = annual_temps.mean().values\n",
        "anomalies = annual_temps - mean_temp\n",
        "\n",
        "# Create climate stripes plot\n",
        "fig, ax = plt.subplots(figsize=(14, 3))\n",
        "\n",
        "# Create a colormap (blue for cold, red for warm)\n",
        "cmap = plt.cm.RdBu_r  # Reversed Red-Blue colormap\n",
        "\n",
        "# Determine color scale limits for symmetry\n",
        "vmax = max(abs(anomalies.min().values), abs(anomalies.max().values))\n",
        "vmin = -vmax\n",
        "\n",
        "# Plot each year as a vertical bar\n",
        "years = anomalies.year.values\n",
        "for i, year in enumerate(years):\n",
        "    ax.bar(year, 1, width=1.0, color=cmap((anomalies.sel(year=year).values - vmin) / (vmax - vmin)),\n",
        "           edgecolor='none')\n",
        "\n",
        "# Remove all axes, labels, and ticks for the iconic climate stripes look\n",
        "ax.set_xlim(years[0] - 0.5, years[-1] + 0.5)\n",
        "ax.set_ylim(0, 1)\n",
        "ax.axis('off')\n",
        "\n",
        "# Add title\n",
        "ax.set_title(f'Warming Stripes for {LAT}°N, {LON}°E',\n",
        "             fontweight='bold', pad=10)\n",
        "\n",
        "plt.tight_layout(pad=0.5)\n",
        "plt.savefig('warming_stripes.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"Plot saved as 'warming_stripes.png'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POb_RSZ6LotS"
      },
      "source": [
        "---\n",
        "\n",
        "### Example 2: Global warming patterns\n",
        "\n",
        "Next, let's calculate the spatial patterns associated with the change in the 2m temperature. To do this, we will calculate the difference between the 2m temperature averaged over the first and last decade in the ERA5 datset. This will allow us to illustrate xarray's *groupby* operation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oI6I9xtvLotS"
      },
      "outputs": [],
      "source": [
        "# Define the decades to compare\n",
        "FIRST_DECADE = (1940, 1949)\n",
        "LAST_DECADE = (2015, 2024)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kavjHERfLotS"
      },
      "outputs": [],
      "source": [
        "# Download global 2m temperature data for both decades\n",
        "dataset = \"reanalysis-era5-single-levels-monthly-means\"\n",
        "\n",
        "# Create list of years for both decades\n",
        "years_first = [str(y) for y in range(FIRST_DECADE[0], FIRST_DECADE[1] + 1)]\n",
        "years_last = [str(y) for y in range(LAST_DECADE[0], LAST_DECADE[1] + 1)]\n",
        "all_years = years_first + years_last\n",
        "\n",
        "months = [f\"{m:02d}\" for m in range(1, 13)]\n",
        "\n",
        "request = {\n",
        "    \"product_type\": \"monthly_averaged_reanalysis\",\n",
        "    \"variable\": [\"2m_temperature\"],\n",
        "    \"year\": all_years,\n",
        "    \"month\": months,\n",
        "    \"time\": \"00:00\",\n",
        "    \"data_format\": \"netcdf\",\n",
        "    \"download_format\": \"unarchived\"\n",
        "}\n",
        "\n",
        "print(f\"Downloading global 2m temperature data...\")\n",
        "print(f\"  Years: {FIRST_DECADE[0]}-{FIRST_DECADE[1]} and {LAST_DECADE[0]}-{LAST_DECADE[1]}\")\n",
        "print(\"  Coverage: Global\")\n",
        "print(\"  (This may take several minutes for global data)\")\n",
        "\n",
        "global_temp_data = earthkit.data.from_source(\"cds\", dataset, request).to_xarray()\n",
        "\n",
        "print(\"Done downloading data\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JMfwLxCLotS"
      },
      "source": [
        "Next, we will use xarray's groupby operation to average the data over each decade. By doing the groupby operation first to group by decade and then nesting a mean over the time dimension, we can obtain the decadal average."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9vfW3OpgLotS"
      },
      "outputs": [],
      "source": [
        "# Extract 2m temperature\n",
        "temp_global = global_temp_data.t2m\n",
        "\n",
        "# relabel the time dimension to 'time' for convenience\n",
        "temp_global = temp_global.rename({'valid_time': 'time'})\n",
        "\n",
        "# Add a 'decade' coordinate to group data by decade\n",
        "# This demonstrates xarray's powerful groupby operation\n",
        "years = temp_global['time'].dt.year\n",
        "decade_labels = xr.where(\n",
        "    (years >= FIRST_DECADE[0]) & (years <= FIRST_DECADE[1]),\n",
        "    'first_decade',\n",
        "    'last_decade'\n",
        ")\n",
        "temp_with_decade = temp_global.assign_coords(decade=('time', decade_labels.data))\n",
        "\n",
        "# Use groupby to calculate the average temperature for each decade\n",
        "# This is a key xarray operation for aggregating data\n",
        "decadal_avg = temp_with_decade.groupby('decade').mean(dim='time')\n",
        "\n",
        "# Extract the two decades\n",
        "first_decade_temp = decadal_avg.sel(decade='first_decade')\n",
        "last_decade_temp = decadal_avg.sel(decade='last_decade')\n",
        "\n",
        "# Calculate the warming (temperature difference)\n",
        "warming = last_decade_temp - first_decade_temp\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpKPFOOZLotT"
      },
      "source": [
        "Finally, let's plot a map of the net warming. This shows that the warming trend is not spatially uniform. In particular notice the amplification in the warming signal in the Arctic region. There has also been more warming in dry regions (with less buffering from the heat capacity of water)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKJAOYWPLotT"
      },
      "outputs": [],
      "source": [
        "# Create figure with Robinson projection\n",
        "fig = plt.figure(figsize=(14, 8))\n",
        "ax = fig.add_subplot(1, 1, 1, projection=ccrs.Robinson())\n",
        "\n",
        "# Common plotting parameters\n",
        "vmin, vmax = -5, 5  # Temperature change range in °C\n",
        "cmap = 'RdBu_r'  # Blue for cooling, white at zero, red for warming\n",
        "\n",
        "# Create a diverging norm centered at zero (so white is at 0°C change)\n",
        "import matplotlib.colors as mcolors\n",
        "norm = mcolors.TwoSlopeNorm(vmin=vmin, vcenter=0, vmax=vmax)\n",
        "\n",
        "# Plot the warming data\n",
        "im = ax.pcolormesh(\n",
        "    warming.longitude,\n",
        "    warming.latitude,\n",
        "    warming.values,\n",
        "    transform=ccrs.PlateCarree(),\n",
        "    cmap=cmap,\n",
        "    norm=norm,\n",
        "    shading='auto'\n",
        ")\n",
        "\n",
        "# Add coastlines and features\n",
        "ax.add_feature(cfeature.COASTLINE, linewidth=0.5, edgecolor='black')\n",
        "ax.add_feature(cfeature.BORDERS, linewidth=0.3, linestyle=':', edgecolor='gray')\n",
        "\n",
        "# Add gridlines\n",
        "gl = ax.gridlines(draw_labels=False, linewidth=0.5, color='gray',\n",
        "                  alpha=0.3, linestyle='--')\n",
        "\n",
        "# Add colorbar\n",
        "cbar = plt.colorbar(im, ax=ax, orientation='horizontal', pad=0.05, shrink=0.8, extend='both')\n",
        "cbar.set_label(f'Temperature Change (°C): {FIRST_DECADE[0]}s to {LAST_DECADE[0]}s')\n",
        "\n",
        "# Title\n",
        "ax.set_title(f'Decadal mean temperature change: ({LAST_DECADE[0]}-{LAST_DECADE[1]}) - ({FIRST_DECADE[0]}-{FIRST_DECADE[1]})',\n",
        "             fontweight='bold', pad=15)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('global_warming_map.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n Plot saved as 'global_warming_map.png'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zm8bNrB9LotT"
      },
      "source": [
        "---\n",
        "\n",
        "### Example 3: Extreme precipitation\n",
        "\n",
        "In this example, we will find the wettest month in the UK and explore the weather patterns associated with that month. To do that, we will use the regionmask and geopandas packages to average the precipitation over all lat/lon values that are within the UK. You can use these packages to find analyze the climate of other countries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sAPogdcsLotT"
      },
      "outputs": [],
      "source": [
        "# Define the time period\n",
        "START_YEAR = 1940\n",
        "END_YEAR = 2024\n",
        "\n",
        "# Use regionmask to get country boundaries from Natural Earth data\n",
        "countries = regionmask.defined_regions.natural_earth_v5_0_0.countries_110\n",
        "\n",
        "# Find the UK index\n",
        "UK_INDEX = 143  # United Kingdom in Natural Earth dataset\n",
        "\n",
        "# Define a bounding box for downloading (we'll mask to UK after)\n",
        "# This needs to be slightly larger than the UK to ensure we capture all coastal areas\n",
        "UK_BBOX = [62, -12, 48, 4]  # [North, West, South, East] - covers UK with margin\n",
        "\n",
        "# North Atlantic region for sea level pressure analysis [North, West, South, East]\n",
        "NORTH_ATLANTIC_REGION = [70, -80, 10, 20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7FYCkCVLotT"
      },
      "outputs": [],
      "source": [
        "# Download ERA5 monthly total precipitation for the UK bounding box\n",
        "dataset = \"reanalysis-era5-single-levels-monthly-means\"\n",
        "\n",
        "years = [str(y) for y in range(START_YEAR, END_YEAR + 1)]\n",
        "months = [f\"{m:02d}\" for m in range(1, 13)]\n",
        "\n",
        "request = {\n",
        "    \"product_type\": \"monthly_averaged_reanalysis\",\n",
        "    \"variable\": [\"total_precipitation\"],\n",
        "    \"year\": years,\n",
        "    \"month\": months,\n",
        "    \"time\": \"00:00\",\n",
        "    \"area\": UK_BBOX,  # Bounding box covering UK bounding box\n",
        "    \"data_format\": \"netcdf\",\n",
        "    \"download_format\": \"unarchived\"\n",
        "}\n",
        "\n",
        "print(f\"Downloading monthly precipitation data for UK bounding box...\")\n",
        "print(f\"  Years: {START_YEAR}-{END_YEAR}\")\n",
        "print(f\"  Bounding box: {UK_BBOX}\")\n",
        "print(\"  (This may take 1-2 minutes)\")\n",
        "\n",
        "precip_data = earthkit.data.from_source(\"cds\", dataset, request).to_xarray()\n",
        "\n",
        "print(f\"Done downloading data\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOE_Owt7LotT"
      },
      "outputs": [],
      "source": [
        "# Now, find the wettest month in the historical record\n",
        "\n",
        "# Extract precipitation\n",
        "precip = precip_data.tp\n",
        "\n",
        "# relabel the time dimension to 'time' for convenience\n",
        "precip = precip.rename({'valid_time': 'time'})\n",
        "\n",
        "# Convert from meters to millimeters (ERA5 gives precipitation in meters)\n",
        "precip_mm = precip * 1000\n",
        "\n",
        "# Create a mask using regionmask to identify UK grid points\n",
        "# This uses actual country boundaries from Natural Earth data\n",
        "uk_mask = countries.mask(precip_mm.longitude, precip_mm.latitude)\n",
        "\n",
        "# Apply the mask: set non-UK points to NaN\n",
        "precip_uk_only = precip_mm.where(uk_mask == UK_INDEX)\n",
        "\n",
        "# Average over the UK region (spatial average, ignoring NaN)\n",
        "# This gives us the UK-averaged precipitation for each month\n",
        "precip_uk_avg = precip_uk_only.mean(dim=['latitude', 'longitude'])\n",
        "\n",
        "# Find the wettest month\n",
        "max_precip_idx = int(precip_uk_avg.argmax())\n",
        "wettest_date = precip_uk_avg['time'].values[max_precip_idx]\n",
        "max_precip_value = float(precip_uk_avg.values[max_precip_idx])\n",
        "\n",
        "# Convert numpy datetime64 to Python datetime\n",
        "wettest_date_dt = wettest_date.astype('M8[ms]').astype('O')\n",
        "wettest_year = wettest_date_dt.year\n",
        "wettest_month = wettest_date_dt.month\n",
        "\n",
        "print(\"\\nWETTEST MONTH on record for UK:\")\n",
        "print(f\"   Date: {wettest_date_dt.strftime('%B %Y')}\")\n",
        "print(f\"   UK-averaged precipitation: {max_precip_value:.1f} mm\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtlPARJxLotT"
      },
      "source": [
        "Let's try to gain some insight into what was happening in the atmosphere that led to these extreme conditions. To do this, we will download the sea level pressure for the February across the full ERA5 dataset.  That will let us compare with 2020 to see what patterns in sea level pressure might have led to this particularly wet month. Note that we download data for a broader North Atlantic region to see the large-scale atmospheric circulation pattern associated with this month."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BEtVpqsBLotT"
      },
      "outputs": [],
      "source": [
        "# Now, download mean sea level pressure data for the wettest month on record\n",
        "dataset = \"reanalysis-era5-single-levels-monthly-means\"\n",
        "\n",
        "years = [str(y) for y in range(START_YEAR, END_YEAR + 1)]\n",
        "month_str = f\"{wettest_month:02d}\"\n",
        "\n",
        "request = {\n",
        "    \"product_type\": \"monthly_averaged_reanalysis\",\n",
        "    \"variable\": [\"mean_sea_level_pressure\"],\n",
        "    \"year\": years,\n",
        "    \"month\": [month_str],\n",
        "    \"time\": \"00:00\",\n",
        "    \"area\": NORTH_ATLANTIC_REGION,\n",
        "    \"data_format\": \"netcdf\",\n",
        "    \"download_format\": \"unarchived\"\n",
        "}\n",
        "\n",
        "print(f\"Downloading mean sea level pressure data for month {wettest_month} ({wettest_date_dt.strftime('%B')})\")\n",
        "print(f\"  Years: {START_YEAR}-{END_YEAR}\")\n",
        "print(f\"  Region: {NORTH_ATLANTIC_REGION}\")\n",
        "print(\"  (This may take several minutes for a large region)\")\n",
        "\n",
        "mslp_data = earthkit.data.from_source(\"cds\", dataset, request).to_xarray()\n",
        "\n",
        "# relabel the time dimension to 'time' for convenience\n",
        "mslp_data = mslp_data.rename({'valid_time': 'time'})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBaCmI71LotT"
      },
      "source": [
        "Now, calculate the montly average (climatology) of the sea level pressure for this month across all years in the dataset. Then we can calculate the pressure anomaly to visualize how the wettest month on record was different from the average for the same month (February in this case)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sn2IEui1LotT"
      },
      "outputs": [],
      "source": [
        "# Extract mean sea level pressure and convert from Pa to hPa for easier interpretation\n",
        "mslp = mslp_data.msl / 100.0  # Pa to hPa\n",
        "\n",
        "# Calculate climatology (mean across all years for this month)\n",
        "mslp_climatology = mslp.mean(dim='time')\n",
        "\n",
        "# Extract sea level pressure for the wettest year\n",
        "# Find the index corresponding to the wettest year\n",
        "mslp_times = mslp['time'].values\n",
        "wettest_idx = None\n",
        "for i, t in enumerate(mslp_times):\n",
        "    dt = t.astype('M8[ms]').astype('O')\n",
        "    if dt.year == wettest_year:\n",
        "        wettest_idx = i\n",
        "        break\n",
        "\n",
        "if wettest_idx is not None:\n",
        "    mslp_wettest_year = mslp.isel({'time': wettest_idx})\n",
        "    # Calculate anomaly (departure from climatology)\n",
        "    mslp_anomaly = mslp_wettest_year - mslp_climatology\n",
        "else:\n",
        "    print(f\"Error: Could not find sea level pressure data for {wettest_year}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0BC4pq0LotX"
      },
      "source": [
        "Now we'll create a map showing the sea level pressure anomaly pattern during the wettest month. This will reveal the large-scale atmospheric circulation pattern associated with the extreme precipitation event.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4DZjprB-LotY"
      },
      "outputs": [],
      "source": [
        "# Create figure\n",
        "fig = plt.figure(figsize=(14, 10))\n",
        "ax = fig.add_subplot(1, 1, 1, projection=ccrs.PlateCarree())\n",
        "\n",
        "\n",
        "# Set map extent to North Atlantic region\n",
        "ax.set_extent([NORTH_ATLANTIC_REGION[1], NORTH_ATLANTIC_REGION[3],\n",
        "               NORTH_ATLANTIC_REGION[2], NORTH_ATLANTIC_REGION[0]],\n",
        "              crs=ccrs.PlateCarree())\n",
        "\n",
        "# Plot sea level pressure anomaly (shaded)\n",
        "im = ax.pcolormesh(\n",
        "    mslp_anomaly.longitude,\n",
        "    mslp_anomaly.latitude,\n",
        "    mslp_anomaly.values,\n",
        "    transform=ccrs.PlateCarree(),\n",
        "    cmap='RdBu_r',\n",
        "    vmin=-30, vmax=30  # Adjusted for pressure anomalies in hPa\n",
        ")\n",
        "\n",
        "# Add contour lines of climatological pressure (to show typical patterns)\n",
        "contours = ax.contour(\n",
        "    mslp_climatology.longitude,\n",
        "    mslp_climatology.latitude,\n",
        "    mslp_climatology.values,\n",
        "    levels=10,\n",
        "    colors='black',\n",
        "    linewidths=1,\n",
        "    alpha=0.4,\n",
        "    transform=ccrs.PlateCarree()\n",
        ")\n",
        "ax.clabel(contours, inline=True, fmt='%.0f hPa')\n",
        "\n",
        "# Add map features\n",
        "ax.add_feature(cfeature.COASTLINE, linewidth=0.8, edgecolor='#333333')\n",
        "ax.add_feature(cfeature.BORDERS, linewidth=0.5, linestyle=':', edgecolor='#666666')\n",
        "ax.add_feature(cfeature.LAND, facecolor='#f5f5f5', alpha=0.7)\n",
        "\n",
        "# Add gridlines\n",
        "gl = ax.gridlines(draw_labels=True, linewidth=0.5, color='gray',\n",
        "                  alpha=0.5, linestyle='--')\n",
        "gl.top_labels = False\n",
        "gl.right_labels = False\n",
        "\n",
        "# Add colorbar\n",
        "cbar = plt.colorbar(im, ax=ax, orientation='horizontal', pad=0.05, shrink=0.8)\n",
        "cbar.set_label('Sea Level Pressure Anomaly (hPa)')\n",
        "\n",
        "# Plot the UK boundary using regionmask\n",
        "# Get the UK region and plot its outline\n",
        "uk_region = countries[[UK_INDEX]]  # Select just the UK\n",
        "uk_region.plot_regions(ax=ax, add_label=False, line_kws={'linewidth': 2, 'color': 'black'})\n",
        "\n",
        "# Title\n",
        "ax.set_title(f\"Sea Level Pressure During UK's Wettest Month, {wettest_date_dt.strftime('%B %Y')} \\n\"\n",
        "             \" (Shaded: Anomaly, Contours: Climatology\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('mslp_anomaly_wettest_month.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"Plot saved as 'mslp_anomaly_wettest_month.png'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6-7ROHsLotY"
      },
      "source": [
        "Notice that the wettest month had relatively low pressure between Greenland and Iceland and relatively high pressure off the coast of Spain and North Africa. If we compare this with the climatogical pressure, we see that this anomaly is associated with a stronger north/south pressure gradient. Geostrophic balance implies that this will lead to a stronger westerly jet. Through geostrophic balance, we can see how distant changes in pressure could lead to extreme conditions at a given location. In the next notebook we will examine these remote climate connections more quantitatively."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}